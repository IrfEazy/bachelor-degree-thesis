In questo capitolo si pone maggiormente l'attenzione sui meccanismi della concorrenza.
In particolare, si affrontano i problemi legati alla condivisione di variabili fra molteplici goroutine, le tecniche analitiche per riconoscere questi problemi e i pattern per risolverli.
Infine verrà esposta la differenza fra goroutine e thread del sistema operativo.


\section{Race condition}
\label{sec:race_condition}%
In un programma sequenziale, ovvero un programma con un'unica goroutine, gli step di un programma vengono eseguiti in un ordine familiare determinato dalla logica del programma.
Per esempio, in una sequenza di istruzioni, il primo viene eseguito prima del secondo, e così via.
In un programma con due o più goroutine, gli step che una goroutine deve eseguire vengono sempre eseguiti in un ordine familiare, ma in generale non è possibile sapere se un evento $x$ in una goroutine accade prima di un evento $y$ in un'altra goroutine, o se accade dopo, o se simultaneamente.
Quando non si conosce l'ordine di esecuzione di due eventi, essi sono detti \textit{concorrenti}.

Considerando una funzione che lavora correttamente in un programma sequenziale, tale funzione è \textit{concurrency-safe} se continua a lavorare correttamente anche se chiamata in modo concorrente.
Si può generalizzare questa definizione ad un insieme di funzioni che si richiamano, come i metodi e le operazioni di un tipo particolare.
Un tipo è concurrency-safe se tutti i suoi metodi e operazioni accessibili sono concurrency-safe.

Si può produrre un programma concurrency-safe senza rendere ogni tipo concreto tale.
Infatti, i tipi concurrency-safe sono l'eccezione piuttosto che la regola, per cui bisogna accedere ad una variabile concorrente solo se la documentazione di quel tipo dice che è sicuro.
Gli accessi concorrenti a molte variabili sono evitate sia \textit{confinandole} in goroutine singole che mantenendo un alto livello di invarianza di \textit{mutua esclusione}.

In contrasto, le funzioni esportate a livello di package ci si aspetta \textit{siano} in generale concurrency-safe.
Fintanto che le variabili a livello di package non sono confinate in una singola goroutine, le funzioni che le modificano devono assicurare la mutua esclusione.

Ci sono molte ragioni per cui una funzione non debba lavorare quando chiamata in modo concorrente, inclusi i deadlock, livelock e la starvation delle risorse.
In questo capitolo verrà solo affrontata la \textit{race condition}.

Una race condition è una situazione in cui il programma non restituisce il corretto risultato per qualche operazione di più goroutine.
Le race condition sono perniciose perché possono rimanere latenti in un programma e apparire poco frequentemente, se non sotto un grosso carico di lavoro o quando usate in certe piattaforme o architetture.
Questo le rende difficili da riprodurre e da diagnosticare.

Prendiamo un tipico esempio per chiarire la serietà del problema:
\begin{lstlisting}[frame=single, label={lst:lstlisting9-1.1}]
var balance int

func Deposit(amount int) { balance = balance + amount }

func Balance() int { return balance }
\end{lstlisting}
Se in questo programma si richiamano le funzioni in modo concorrente, \verb|Balance| non è più garantito essere corretto.
Si considerino le seguenti goroutine:
\begin{lstlisting}[frame=single, label={lst:lstlisting9-1.2}]
// Alice:
go func() {
    bank.Deposit(200)               // A1
    fmt.Println(%*``*\)=%*''*\), bank.Balance()) // A2
}()

// Bob:
go bank.Deposit(100) // B
\end{lstlisting}
Alice deposita \dollar 200, quindi controlla il suo bilancio, mentre Bob deposita \dollar 100.
Fintanto che le istruzioni \verb|A1| e \verb|A2| vengono eseguite in modo concorrente con \verb|B|, si può cadere in errore pensando di avere solo tre scenari (A1+A2+B, A1+B+A2, B+A1+A2).
Il problema è che B può accadere simultaneamente ad A1, e quindi avere un risultato di questo tipo:
\begin{lstlisting}[label={lst:lstlisting9-1.3}]
Data	race
        0
A1r     0       ... = balance + amount
B       100
A1w     200     balance = ...
A2      %*``*\)= 200%*''*\)
\end{lstlisting}
Dopo \verb|A1r|, l'espressione \verb|balance + amount| vale $200$, così questo sarà il valore scritto durante \verb|A1w|, anche se nel frattempo viene eseguito un'altro deposito.
Il bilancio finale è di soli \dollar 200.

Questo programma contiene la race condition detta \textit{data race}.
Un data race accade ogni volta che due goroutine accedono alla stessa variabile in modo concorrente e almeno uno dei due accessi sia una scrittura.

Questo problema diventa ancor più vulnerabile e quindi evidente se i data race coinvolgono una variabile di un tipo più grande, come un'interfaccia, una stringa o una slice.

Il primo modo per evitare una race condition è non fare scritture.
Si consideri la seguente map, popolata in modo pigro in quanto ogni chiave è richiesta per la prima volta.
Se \verb|Icon| è invocato in modo sequenziale, il programma lavora bene, ma se \verb|Icon| è invocato in modo concorrente, allora c'è un data race per l'accesso alla map.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-1.4}]
var icons = make(map[string]image.Image)

func loadIcon(name string) image.Image

// NOTA: non %*\textit{è}*\) concurrency-safe
func Icon(name string) image.Image {
    icon, ok := icons[name]
    if !ok {
        icon = loadIcon(name)
        icons[name] = icon
    }
    return icon
}
\end{lstlisting}
Se invece venisse inizializzata la map con tutte le entry necessarie prima di creare la seconda goroutine e non la si modificasse mai più, allora si avrebbe la concurrency-safe perché non ci sarebbero scritture.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-1.5}]
var icons = map[string]image.Image {
    %*``*\)spades.png%*''*\):   loadIcon(%*``*\)spades.png%*''*\)),
    %*``*\)hearts.png%*''*\):   loadIcon(%*``*\)hearts.png%*''*\)),
    %*``*\)diamonds.png%*''*\): loadIcon(%*``*\)diamonds.png%*''*\)),
    %*``*\)clubs.png%*''*\):    loadIcon(%*``*\)clubs.png%*''*\)),
}

// Concurrency-safe
func Icon(name string) image.Image { return icons[name] }
\end{lstlisting}
Quest'ultima implementazione rende la funzione concurrency-safe perché l'assegnazione alla map viene fatta durante l'inizializzazione del package, che \textit{avviene sempre prima} di eseguire la funzione \verb|main|.

Fintanto che le goroutine non possono accedere in modo diretto alle variabili, esse devono usare i channel per inviare una richiesta alle goroutine confinate per interrogare o aggiornare una loro variabile.
Il mantra di Go è infatti ``Do not communicate by sharing memory;
instead, share memory by communicating''.
Una goroutine che gestisce l'accesso ad una variabile confinata tramite l'uso di richieste su channel è detta \textit{goroutine monitor} per quella variabile.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-1.6}]
var deposits = make(chan int) // invia l'ammontare di deposito
var balances = make(chan int) // riceve il bilancio

func Deposit(amount int) { deposits <- amount }
func Balance() int       { return <-balances }

func teller() {
    var balance int // balance %*\textit{è}*\) confintato alla goroutine teller
    for {
        select {
        case amount := <-deposits:
            balance += amount
        case balances <- balance:
        }
    }
}

func init() {
    go teller() // avvia la goroutine monitor
}
\end{lstlisting}
Anche se la variabile non può essere confinata da una singola goroutine per l'intera sua vita, il confinamento può comunque essere una soluzione al problema per l'accesso concorrente.


\section{Mutua esclusione: sync.Mutex}
\label{sec:mutua_esclusione_syncmutex}%
Possiamo pensare di usare un channel con capacity pari a $1$ per assicurarsi che al più una goroutine alla volta acceda ad una variabile condivisa.
Un semaforo che conta fino ad $1$ è detto \textit{semaforo binario}.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-2.1}]
var (
    sema    = make(chan struct{}, 1) // un semaforo binario a
                                     // fare la guardia a
                                     // balance
    balance int
)
\end{lstlisting}
\begin{lstlisting}[frame=single, label={lst:lstlisting9-2.2}]
func Deposit(amount int) {
    sema <- struct{}{} // acquisizione token
    balance = balance + amount
    <-sema // rilascio token
}

func Balance() int {
    sema <- struct{}{} // acquisizione token
    b := balance
    <- sema // rilascio token
    return b
}
\end{lstlisting}
Questo pattern di \textit{mutua esclusione} è talmente utile da essere supportato direttamente dal tipo \verb|Mutex| del package \verb|sync|.
Il metodo \verb|Lock| acqusisce un token (detto un \textit{lock}) e il suo metodo \verb|Unlock| lo rilascia:
\begin{lstlisting}[frame=single, label={lst:lstlisting9-2.3}]
var (
    mu      sync.Mutex // fa la guardia a balance
    balance int
)

func Deposit(amount int) {
    mu.Lock()
    balance = balance + amount
    mu.Unlock()
}

func Balance() int {
    mu.Lock()
    b := balance
    mu.Unlock()
    return b
}
\end{lstlisting}
Ogni volta che una goroutine accede alle variabili di una banca, esso deve richiamare il metodo \verb|Lock| per acquisire l'esclusivo lock.
Se un'altra goroutine ha acquisito il lock, questa operazione si bloccherà quando l'altra goroutine richiamerà \verb|Unlock| e il lock diventerà disponibile nuovamente.
Il mutex \textit{fa la guardia} alle variabili condivise.
Per convenzione, le variabili difese da un mutex sono dichiarate immediatamente dopo la dichiarazione del mutex stesso.
Se si devia da questa prassi, bisogna documentarla.

La regione di codice fra \verb|Lock| e \verb|Unlock| in cui una goroutine è libera di leggere e modificare una variabile condivisa è detta \textit{sezione critica}.
La chiamata di \verb|Unlock| da parte del possessore del lock avviene prima che un'altra goroutine possa acquisire il lock per sè.
L'insieme delle funzioni, dei lock dei mutex e delle variabili è detto \textit{monitor}.

In sezioni critiche complesse, specialmente quelle che possono lanciare errori e quindi restituire un risultato in modo prematuro, il metodo \verb|Unlock| potrebbe non essere raggiunto.
L'istruzione \verb|defer| di Go permette di chiamare \verb|Unlock| in modo differito, in questo modo il programmatore non ha più il dovere di capire dove andare ad inserire il metodo perché tanto la funzione differita sarà sempre l'ultima operazione eseguita dalla funzione.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-2.4}]
func Balance() int {
    mu.Lock()
    defer mu.Unlock()
    return balance
}
\end{lstlisting}
Con l'istruzione \verb|defer| si elimina anche la necessità di una variabile locale.

Si consideri ora la seguente funzione \verb|Withdraw| che riduce il bilancio di un ammontare specificato e restituisce \verb|true|, mentre se l'account non possiede sufficienti fondi per la transazione, \verb|Withdraw| ripristina il bilancio e restituisce \verb|false|.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-2.5}]
// NOTA: non atomica!
func Withdraw(amount int) bool {
    Deposit(-amount)
    if Balance() < 0 {
        Deposit(amount)
        return false // fondi insufficienti
    }
    return true
}
\end{lstlisting}
Oltre a restituire un risultato incorretto, questa funzione ha un brutto effetto collaterale.
Quando si tenta un prelievo di una somma eccessiva per il bilancio, questo va in negativo.
Questo potrebbe causare ad un prelievo concorrente di piccola somma di essere rifiutato.
Questo problema è dato perché \verb|Withdraw| non è una funzione \textit{atomica}: è la composizione di tre operazioni atomiche che a turno prendono il lock e lo rilasciano, ma niente blocca l'intera sequenza.

Idealmente, \verb|Withdraw| dovrebbe acquisire il lock del mutex una volta per tutta l'operazione.
Comunque, questo tentativo fallirebbe perché \verb|Balance()| rimarrebbe in attesa del lock per sempre, andando quindi in deadlock.

Un modo per superare il problema è dividere le funzioni in due parti: una funzione non esportata \verb|deposit|, che richiede come ipotesi il lock sia già applicato ed esegue il lavoro, e una funzione esportata \verb|Deposit| che acquisisce il lock prima di chiamare \verb|deposit|.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-2.6}]
func Withdraw(amount int) bool {
    mu.Lock()
    defer mu.Unlock()
    deposit(-amount)
    if balance < 0 {
        deposit(amount)
        return false // fondi insufficienti
    }
    return true
}

func Deposit(amount int) {
    mu.Lock()
    defer mu.Unlock()
    deposit(amount)
}
\end{lstlisting}
\begin{lstlisting}[frame=single, label={lst:lstlisting9-2.7}]
func Balance() {
    mu.Lock()
    defer mu.Unlock()
    return balance
}

// Questa funzione richiede che il lock sia gi%*\textit{à}*\) applicato
func deposit(amount int) { balance += amount }
\end{lstlisting}
Quando si usa un mutex, bisogna essere sicuri che sia questo che la variabile che questo controlla siano non esportati, sia che siano variabili a livello di package che campo di una struttura.


\section{Mutex di lettura/scrittura: sync.RWMutex}
\label{sec:mutex_di_lettura_scrittura_syncrwmutex}%
Per permettere alle operazioni di sola lettura di essere eseguite in parallelo, e solo alle operazioni di scrittura di avere completo ed esclusivo accesso alle variabili, si può utilizzare il mutex \verb|sync.RWMutex|, per poter utilizzare il lock detto \textit{multiple readers, single writer} lock:
\begin{lstlisting}[frame=single, label={lst:lstlisting9-3.1}]
var mu sync.RWMutex
var balance int

func Balance() int {
    mu.RLock() // reader lock
    defer mu.RUnclock()
    return balance
}
\end{lstlisting}
La funzione \verb|Balance| ora chiama i metodi \verb|RLock| e \verb|RUnlock| per acquisire e rilasciare un \textit{reader} o \textit{shared} lock.
La funzione \textit{Deposit} chiamerà sempre i metodi \verb|mu.Lock| e \verb|mu.Unlock| per acquisire e rilasciare un \textit{writer} o \textit{exclusive} lock.


\section{Sincronizzazione di memoria}
\label{sec:sincronizzazione_di_memoria}%
Nei computer moderni ci sono una dozzina di processori, ognuno con la propria cache locale.
Per efficienza, le scritture alla memoria sono propagate solo all'interno del processore e caricate in memoria centrale solo quando necessarie.
Tali scritture possono anche essere caricate in memoria in ordine diverso rispetto a come sono state scritte nella goroutine.
Le primitive di sincronizzazione come i channel di comunicazione e le operazioni di mutex fanno sì che il processore carichi tutte le scritture accumulate cosicché gli effetti dell'esecuzione della goroutine siano garantiti essere visibili alle goroutine in esecuzione sugli altri processori.

Si consideri l'output del seguente codice:
\begin{lstlisting}[frame=single, label={lst:lstlisting9-4.1}]
var x, y int
go func() {
    x = 1                  // A1
    fmt.Print(%*``*\)y:%*''*\), y, %*``*\) %*''*\)) // A2
}()
go func() {
    y = 1                  // B1
    fmt.Print(%*``*\)x:%*''*\), x, %*``*\) %*''*\)) // B2
}()
\end{lstlisting}
Dato che le due goroutine sono concorrenti e accedono alle variabili condivise senza mutua esclusione, c'è un data race, per cui il programma non può essere deterministico.

I possibili output:
\begin{lstlisting}[language=bash, frame=L, label={lst:lstlisting9-4.2}]
y:0 x:1
x:0 y:1
x:1 y:1
y:1 x:1
\end{lstlisting}
La quarta linea può essere spiegata con la sequenza \verb|A1+B1+A2+B2| o \verb|B1+A1+A2+B2|, per esempio.

Comunque, queste due sequenze possono avere output:
\begin{lstlisting}[language=bash, frame=L, label={lst:lstlisting9-4.3}]
x:0 y:0
y:0 x:0
\end{lstlisting}
Ma dipende dal compilatore, dalla CPU e da tanti altri fattori.
Tutti questi problemi possono essere evitati con un uso consistente di semplici pattern.


\section{Esempio: cache concorrente non-bloccante}
\label{sec:esempio_cache_concorrente_nonbloccante}%
In questa ultima sezione verrà costruita una cache concorrente non-bloccante, un'astrazione che risolve i problemi che sorgono spesso in programmi concorrenti del mondo reale, ma non ben indirizzato da librerie esistenti.
Questo è il problema della \textit{memoizzazione} di una funzione, ovvero di memorizzare nella cache il risultato di una funzione così da non doverla computare una seconda volta se di nuovo necessario.
La soluzione che verrà proposta è concurrency-safe ed evita la contesa di un singolo lock per l'intera cache.

La seguente funzione \verb|httpGetBody| sarà presa come colei che si vuole memoizzare.
Essa produce una richiesta HTTP GET e legge il corpo della risposta.
Chiamate a questa funzione sono relativamente onerose, per cui si vuole evitare di ripeterle se non necessario.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.1}]
func httpGetBody(url string) (interface{}, error) {
    resp, err := http.Get(url)
    if err != nil {
        return nil, err
    }
    defer resp.Body.Close()
    return ioutil.ReadAll(resp.Body)
}
\end{lstlisting}
L'ultima istruzione nasconde un piccola sottigliezza.
\verb|ReadAll| restituisce due valori, un \verb|[]byte| e un \verb|error|, ma fintanto che questi sono assegnabili al tipo dichiarato del risultato di \verb|httpGetBody|, allora si può restituire il risultato della chiamata senza creare variabili locali.

Una prima bozza di cache:
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.2}]
package memo

// Memo tiene nella cache i risultati della chiamata a Func.
type Memo struct {
    f     Func
    cache map[string]result
}

// Func %*\textit{è}*\) il tipo della funzione da memoizzare.
type Func func(key string) (interface{}, error)
\end{lstlisting}
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.3}]
type result struct {
    value interface{}
    err   error
}

func New(f Func) *Memo {
    return &Memo{f: f, cache: make(map[string]result)}
}

// NOTA: non concurrency-safe!
func (memo *Memo) Get(key string) (interface{}, error) {
    res, ok := memo.cache[key]
    if !ok {
        res.Value, res.err = memo.f(key)
        memo.cache[key] = res
    }
    return res.Value, res.err
}
\end{lstlisting}
Un'istanza di \verb|Memo| detiene la funzione \verb|f| da memoizzare, di tipo \verb|Func|, e la cache, che mappa da \verb|string| a \verb|result|.
Ogni \verb|result| è semplicemente la coppia di risultati restituiti dalla chiamata a \verb|f| (un valore e un errore).

Un esempio su come usare \verb|Memo|.
Per ogni elemento di uno stream di un URL si chiama \verb|Get|, tracciando la latenza della chiamata e l'ammontare di dati restituiti dalla funzione:
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.4}]
m := Memo.New(httpGetBody)
for url := range incomingURLs() {
    start := time.Now()
    value, err := m.Get(url)
    if err != nil {
        log.Print(err)
        continue
    }
    fmt.Printf(%*``*\)%s, %s, %d bytes\n%*''*\), url, time.Since(start),
        len(value.([]byte)))
}
\end{lstlisting}
Dato che le richieste HTTP sono un'opportunità per il parallelismo, si può cambiare il test così da rendere tutte le richieste concorrenti.
Per sapere quando l'ultima goroutine si conclude, bisogna incrementare un contatore prima di avviare ogni goroutine e decrementarla ogni volta che finisce una di loro.

Questo servizio è offerto dal tipo contatore \verb|sync.WaitGroup|.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.5}]
m := Memo.New(httpGetBody)
var n sync.WaitGroup
for url := range incomingURLs() {
    n.Add(1)
    go func(url string) {
        defer n.Done()
        start := time.Now()
        value, err := m.Get(url)
        if err != nil {
            log.Print(err)
            return
        }
        fmt.Printf(%*``*\)%s, %s, %d bytes\n%*''*\),
            url, time.Since(start), len(value.([]byte)))
    }(url)
}
n.Wait()
\end{lstlisting}
Eseguendo questo programma più volte, si può notare come siano presenti errori.
La ragione è da ritrovare nel metodo \verb|Get| di \verb|*Memo|.

Il modo più semplice per rendere la cache concurrency-safe è usare una sincronizzazione con i monitor.
Tutto ciò che serve è aggiungere un mutex a \verb|Memo|, acquisire il lock all'inizio di \verb|Get| e quindi rilasciarlo prima di restituire il risultato:
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.6}]
type Memo struct {
    f     Func
    mu    sync.Mutex // fa la guardia alla cache
    cache map[string]result
}

// Get %*\textit{è}*\) concurrency-safe.
func (memo *Memo) Get(key string)
    (value interface{}, err error) {
        memo.mu.Lock()
        res, ok := memo.cache[key]
        if !ok {
            res.value, res.err = memo.f(key)
            memo.cache[key] = res
        }
        memo.mu.Unlock()
        return res.value, res.err
}
\end{lstlisting}
Applicando il lock durante la chiamata a \verb|f|, \verb|Get| serializza tutte le operazioni di I/O che si vogliono parallelizzare.
Ciò che bisogna raggiungere è una cache non-bloccante, una che non serializzi le chiamate alle funzioni da memoizzare.

Nella prossima implementazione di \verb|Get|, la goroutine chiamante acquisisce il lock due volte: una volta per il lookup e poi per l'aggiornamento nel caso il lookup ha restituito nulla.
Nel mezzo le goroutine sono libere di usare la cache.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.7}]
func (memo *Memo) Get(key string)
    (value interface{}, err error) {
        memo.mu.Lock()
        res, ok := memo.cache[key]
        memo.mu.Unlock()
        if !ok {
            res.value, res.err = memo.f(key)

            // Fra le due sezioni critiche, molte goroutine
            // potrebbero competere per computare f(key) e
            // aggiornare la map.
            memo.mu.Lock()
            memo.cache[key] = res
            memo.mu.Unlock()
        }
        return res.value, res.err
}
\end{lstlisting}
Le prestazioni sono migliorate, ma ora alcuni URL sono caricati due volte.
Questo succede quando due o più goroutine chiamano \verb|Get| per la stessa URL allo stesso istante.
Entrambi consultano la cache, non trovano il valore e quindi chiamano la funzione lenta \verb|f|.
Quindi entrambi aggiornano la map con il risultato da loro ottenuto.
Uno dei due risultati viene sovrascritto dall'altro.

Idealmente è meglio evitare lavoro ridondante (\textit{soppressione dei duplicati}).
Nell'ultima versione di \verb|Memo|, ogni elemento di map è un puntatore ad una \verb|entry| struct.
Ogni \verb|entry| contiene il risultato memoizzato di una chiamata alla funzione \verb|f|, ma in aggiunta contiene un channel chiamato \verb|ready|.
Non appena il \verb|result| di \verb|entry| è stato impostato, questo channel viene chiuso, e mandato in \textit{broadcast} a qualunque altra goroutine, per i quali da quel momento sarà sicuro leggere il risultato dalla \verb|entry|.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.8}]
type entry struct {
    res   result
    ready chan struct{} // chiuso quando res %*\textit{è}*\) pronto
}

func New(f Func) *Memo {
    return &Memo{f: f, cache: make(map[string]*entry)}
}

type Memo struct {
    f     Func
    mu    sync.Mutex // fa la guardia a cache
    cache map[string]*entry
}
\end{lstlisting}
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.9}]
func (memo *Memo) Get(key string)
    (value interface{}, err error) {
        memo.mu.Lock()
        e := memo.cache[key]
        if e == nil {
            // Questo %*\textit{è}*\) la prima richiesta per key.
            // Questa goroutine diventa responsabile per la
            // computazione del valore e di mandare in broadcast
            // la condizione ready.
            e = &entry{ready: make(chan struct{})
            memo.cache[key] = e
            memo.mu.Unlock()

            e.res.value, e.res.err = memo.f(key)

            close(e.ready) // invia in broadcast la condizione
                           // di ready
        } else {
            // Questo %*\textit{è}*\) una richiesta ripetuta per key.
            memo.mu.Unlock()

            <-e.ready // attende la condizione di ready
        }
        return e.res.value, e.res.err
}
\end{lstlisting}
Una chiamata a \verb|Get| coinvolge l'acquisizione del lock che fa la guardia a \verb|cache|, la ricerca della map per un puntatore di una \verb|entry| esistente, l'allocazione e l'inserimento di una nuova \verb|entry| se non è stato trovato, e quindi il rilascio del lock.
Se esisteva già una \verb|entry|, il suo valore non è necessariamente pronto, quindi la goroutine chiamante deve attendere la condizione di ready prima di leggere \verb|result| da \verb|entry|.
Lo fa leggendo un valore dal channel \verb|ready|.

Se non esisteva una \verb|entry| allora viene inserito una nuova \verb|entry| non ready nella map, la goroutine corrente diventa responsabile per l'invocazione della funzione lenta, aggiornando la \verb|entry|, e invia in broadcast la nuova \verb|entry| letta alle altre goroutine.

Si noti che \verb|e.res.value| e \verb|e.res.err| sono condivise fra molteplici goroutine.
La goroutine che ha creato la \verb|entry| imposta il loro valore e le altre goroutine leggono il loro valore quando la condizione di ready sarà inviata.
La chiusura del channel \verb|ready| \textit{avviene prima} della ricezione dell'evento da parte delle altre goroutine, quindi la scrittura di queste variabili nella prima goroutine \textit{avviene prima} che essi siano letti da qualunque successiva goroutine.
Non ci sono data race.

L'ultima implementazione di \verb|Memo| usa i mutex a guardia di una variabile map che è condivisa da ogni goroutine che chiama \verb|Get|.
Si veda ora un'altra scelta di implementazione della cache, quella con la variabile map confinata ad una \textit{goroutine monitor} a cui ogni chiamata di \verb|Get| deve inviare un messaggio.

La dichiarazione di \verb|Func|, \verb|result| e \verb|entry| rimane come prima.
Comunque, il tipo \verb|Memo| consiste ora di un channel \verb|requests| tramite il quale il chiamante di \verb|Get| comunica con la goroutine monitor.
Il tipo elementare del channel è una \verb|request|.
Usando questa struttura, il chiamante di \verb|Get| invia alla goroutine monitor sia la chiave che l'argomento della funzione memoizzata, e un altro channel \verb|response| sopra il quale il risultato deve essere inviato indietro quando diventa disponibile.
Questo channel porta solo un singolo valore.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.10}]
// Una richiesta %*\textit{è}*\) un messaggio che richiede che Func sia
// applicato a key.
type request struct {
    key      string
    response chan<- result // il client vuole un risultato
                           // singolo
}

type Memo struct{ requests chan request }

// New ritorna una memoizzazione di f.
// I client devono successivamente chiamare Close.
func New(f Func) *Memo {
    memo := &Memo{requests: make(chan request)}
    go memo.server(f)
    return memo
}

func (memo *Memo) Get(key string) (interface{}, error) {
    response := make(chan result)
    memo.requests <- request{key, response}
    res := <-response
    return res.value, res.err
}

func (memo *Memo) Close() { close(memo.requests) }
\end{lstlisting}
Il metodo \verb|Get| crea un channel di risposta, lo mette nella richiesta, lo invia alla goroutine monitor e quindi riceve da esso.

La variabile di \verb|cache| è confinata alla goroutine monitor \verb|(*Memo).server|.
Il monitor legge richieste in un ciclo fino a quando il channel request viene chiuso dal metodo \verb|Close|.
Per ogni richiesta, esso consulta la cache, crea una nuova \verb|entry| se nessuno la trova e la salva.
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.11}]
func (memo *Memo) server(f Func) {
    cache := make(map[string]*entry)
    for req := range memo.requests {
        e := cache[req.key]
        if e == nil {
            // Questa %*\textit{è}*\) la prima richiesta per key.
            e = &entry{ready: make(chan struct{})}
            cache[req.key] = e
            go e.call(f, req.key) // chiama f(key)
        }
        go e.deliver(req.response)
    }
}
\end{lstlisting}
\begin{lstlisting}[frame=single, label={lst:lstlisting9-7.12}]
func (e *entry) call(f Func, key string) {
    // Valuta la funzione.
    e.res.value, e.res.err = f(key)
    // Invia in broadcast la condizione ready.
    close(e.ready)
}

func (e *entry) deliver(response chan<- result) {
    // Attende per la condizione ready.
    <-e.ready
    // Invia il risultato al client.
    response <- e.res
}
\end{lstlisting}
In un modo simile alla versione basata sul mutex, la prima richiesta per una data chiave diventa responsabile di chiamare la funzione \verb|f| su quella chiave, memorizzando il risultato nella \verb|entry| e facendo il broadcast sulla prontezza della \verb|entry| chiudendo il channel \verb|ready|.
Questo è fatto con \verb|(*entry).call|.

Una successiva richiesta per la stessa chiave trova la \verb|entry| esistente nella map, attende che il risultato diventi ready e invia il risultato tramite la channel di response alla goroutine client che chiama \verb|Get|.
Questo è fatto da \verb|(*entry).deliver|.
I metodi \verb|call| e \verb|deliver| sono chiamati nella loro goroutine per assicurare che la goroutine monitor non fermi il processo di nuove richieste.

Questo esempio mostra che è possibile costruire molte strutture concorrenti usando entrambi gli approcci: variabili condivise e lock o communicating sequential processes.

Non è sempre ovvio quale approccio è preferibile in una data situazione, ma non conviene conoscere come può essere implementato nei due stili perché non è sempre facile capire come tradurre le proprie scelte progettuali in entrambi gli approcci.
Nonostante questo, qualche volta passare da un approccio all'altro può anche rendere il proprio codice più semplice.