\documentclass[../../thesis.tex]{subfiles}
\begin{document}
    \subsection{Esempio: cache concorrente non-bloccante}\label{subsec:esempio:-cache-concorrente-non-bloccante}
    In questa ultima sezione verrà costruita una cache concorrente non-bloccante, un'astrazione che risolve i problemi che sorgono spesso in programmi concorrenti del mondo reale, ma non ben indirizzato da librerie esistenti.
    Questo è il problema della \textit{memoizzazione} di una funzione, ovvero di memorizzare nella cache il risultato di una funzione così da non doverla computare una seconda volta se di nuovo necessario.
    La soluzione che verrà proposta è concurrency-safe ed evita la contesa di un singolo lock per l'intera cache.
    \hfill \vspace{12pt}

    La seguente funzione \verb"httpGetBody" sarà presa come colei che si vuole memoizzare.
    Essa produce una richiesta HTTP GET e legge il corpo della risposta.
    Chiamate a questa funzione sono relativamente onerose, per cui si vuole evitare di ripeterle se non necessario.
    \begin{lstlisting}[frame = single,label={lst:lstlisting9-7.1}]
func httpGetBody(url string) (interface{}, error) {
    resp, err := http.Get(url)
    if err != nil {
        return nil, err
    }
    defer resp.Body.Close()
    return ioutil.ReadAll(resp.Body)
}
    \end{lstlisting}
    L'ultima istruzione nasconde un piccola sottigliezza. \verb"ReadAll" restituisce due valori, un \verb"[]byte" e un \verb"error", ma fintanto che questi sono assegnabili al tipo dichiarato del risultato di \verb"httpGetBody", allora si può restituire il risultato della chiamata senza creare variabili locali.
    \hfill \vspace{12pt}

    Una prima bozza di cache:
    \begin{lstlisting}[frame = single,label={lst:lstlisting9-7.2}]
package memo

// Memo tiene nella cache i risultati della chiamata a Func.
type Memo struct {
    f     Func
    cache map[string]result
}

// Func %*\textit{è}*) il tipo della funzione da memoizzare.
type Func func(key string) (interface{}, error)

type result struct {
    value interface{}
    err   error
}

func New(f Func) *Memo {
    return &Memo{f: f, cache: make(map[string]result)}
}

// NOTA: non concurrency-safe!
func (memo *Memo) Get(key string) (interface{}, error) {
    res, ok := memo.cache[key]
    if !ok {
        res.Value, res.err = memo.f(key)
        memo.cache[key] = res
    }
    return res.Value, res.err
}
    \end{lstlisting}
    Un'istanza di \verb"Memo" detiene la funzione \verb"f" da memoizzare, di tipo \verb"Func", e la cache, che mappa da \verb"string" a \verb"result".
    Ogni \verb"result" è semplicemente la coppia di risultati restituiti dalla chiamata a \verb"f" (un valore e un errore).
    \hfill \vspace{12pt}

    Un esempio su come usare \verb"Memo".
    Per ogni elemento di uno stream di un URL si chiama \verb"Get", tracciando la latenza della chiamata e l'ammontare di dati restituiti dalla funzione:
    \begin{lstlisting}[frame = single,label={lst:lstlisting9-7.3}]
m := Memo.New(httpGetBody)
for url := range incomingURLs() {
    start := time.Now()
    value, err := m.Get(url)
    if err != nil {
        log.Print(err)
        continue
    }
    fmt.Printf("%s, %s, %d bytes\n", url, time.Since(start),
        len(value.([]byte)))
}
    \end{lstlisting}
    Dato che le richieste HTTP sono un'opportunità per il parallelismo, si può cambiare il test così da rendere tutte le richieste concorrenti.
    Per sapere quando l'ultima goroutine si conclude, bisogna incrementare un contatore prima di avviare ogni goroutine e decrementarla ogni volta che finisce una di loro.
    Questo servizio è offerto dal tipo contatore \verb"sync.WaitGroup".
    \begin{lstlisting}[frame = single,label={lst:lstlisting9-7.4}]
m := Memo.New(httpGetBody)
var n sync.WaitGroup
for url := range incomingURLs() {
    n.Add(1)
    go func(url string) {
        defer n.Done()
        start := time.Now()
        value, err := m.Get(url)
        if err != nil {
            log.Print(err)
            return
        }
        fmt.Printf("%s, %s, %d bytes\n",
            url, time.Since(start), len(value.([]byte)))
    }(url)
}
n.Wait()
    \end{lstlisting}
    Eseguendo questo programma più volte, si può notare come siano presenti errori.
    La ragione è da ritrovare nel metodo \verb"Get" di \verb"*Memo".
    \hfill \vspace{12pt}

    Il modo più semplice per rendere la cache concurrency-safe è usare una sincronizzazione con i monitor.
    Tutto ciò che serve è aggiungere un mutex a \verb"Memo", acquisire il lock all'inizio di \verb"Get" e quindi rilasciarlo prima di restituire il risultato:
    \begin{lstlisting}[frame = single,label={lst:lstlisting9-7.5}]
type Memo struct {
    f     Func
    mu    sync.Mutex // fa la guardia alla cache
    cache map[string]result
}

// Get %*\textit{è}*) concurrency-safe.
func (memo *Memo) Get(key string) (value interface{}, err error) {
    memo.mu.Lock()
    res, ok := memo.cache[key]
    if !ok {
        res.value, res.err = memo.f(key)
        memo.cache[key] = res
    }
    memo.mu.Unlock()
    return res.value, res.err
}
    \end{lstlisting}
    Applicando il lock durante la chiamata a \verb"f", \verb"Get" serializza tutte le operazioni di I/O che si vogliono parallelizzare.
    Ciò che bisogna raggiungere è una cache non-bloccante, una che non serializzi le chiamate alle funzioni da memoizzare.
    \hfill \vspace{12pt}

    Nella prossima implementazione di \verb"Get", la goroutine chiamante acquisisce il lock due volte: una volta per il lookup e poi per l'aggiornamento nel caso il lookup ha restituito nulla.
    Nel mezzo le goroutine sono libere di usare la cache.
    \begin{lstlisting}[frame = single,label={lst:lstlisting9-7.6}]
func (memo *Memo) Get(key string) (value interface{}, err error) {
    memo.mu.Lock()
    res, ok := memo.cache[key]
    memo.mu.Unlock()
    if !ok {
        res.value, res.err = memo.f(key)
        
        // Fra le due sezioni critiche, molte goroutine
        // potrebbero competere per computare f(key) e aggiornare la map.
        memo.mu.Lock()
        memo.cache[key] = res
        memo.mu.Unlock()
    }
    return res.value, res.err
}
    \end{lstlisting}
    Le prestazioni sono migliorate, ma ora alcuni URL sono caricati due volte.
    Questo succede quando due o più goroutine chiamano \verb"Get" per la stessa URL allo stesso istante.
    Entrambi consultano la cache, non trovano il valore e quindi chiamano la funzione lenta \verb"f".
    Quindi entrambi aggiornano la map con il risultato da loro ottenuto.
    Uno dei due risultati viene sovrascritto dall'altro.
    \hfill \vspace{12pt}

    Idealmente è meglio evitare lavoro ridondante (\textit{soppressione dei duplicati}).
    Nell'ultima versione di \verb"Memo", ogni elemento di map è un puntatore ad una \verb"entry" struct.
    Ogni \verb"entry" contiene il risultato memoizzato di una chiamata alla funzione \verb"f", ma in aggiunta contiene un channel chiamato \verb"ready".
    Non appena il \verb"result" di \verb"entry" è stato impostato, questo channel viene chiuso, e mandato in \textit{broadcast} a qualunque altra goroutine, per i quali da quel momento sarà sicuro leggere il risultato dalla \verb"entry".
    \begin{lstlisting}[frame = single,label={lst:lstlisting9-7.7}]
type entry struct {
    res   result
    ready chan struct{} // chiuso quando res %*\textit{è}*) pronto
}

func New(f Func) *Memo {
    return &Memo{f: f, cache: make(map[string]*entry)}
}

type Memo struct {
    f     Func
    mu    sync.Mutex // fa la guardia a cache
    cache map[string]*entry
}

func (memo *Memo) Get(key string) (value interface{}, err error) {
    memo.mu.Lock()
    e := memo.cache[key]
    if e == nil {
        // Questo %*\textit{è}*) la prima richiesta per key.
        // Questa goroutine diventa responsabile per la computazione
        // del valore e di mandare in broadcast la condizione ready.
        e = &entry{ready: make(chan struct{})
        memo.cache[key] = e
        memo.mu.Unlock()
        
        e.res.value, e.res.err = memo.f(key)
        
        close(e.ready) // invia in broadcast la condizione di ready
    } else {
        // Questo %*\textit{è}*) una richiesta ripetuta per key.
        memo.mu.Unlock()
        
        <-e.ready // attende la condizione di ready
    }
    return e.res.value, e.res.err
}
    \end{lstlisting}
    Una chiamata a \verb"Get" coinvolge l'acquisizione del lock che fa la guardia a \verb"cache", la ricerca della map per un puntatore di una \verb"entry" esistente, l'allocazione e l'inserimento di una nuova \verb"entry" se non è stato trovato, e quindi il rilascio del lock.
    Se esisteva già una \verb"entry", il suo valore non è necessariamente pronto, quindi la goroutine chiamante deve attendere la condizione di ready prima di leggere \verb"result" da \verb"entry".
    Lo fa leggendo un valore dal channel \verb"ready".
    \hfill \vspace{12pt}

    Se non esisteva una \verb"entry" allora viene inserito una nuova \verb"entry" non ready nella map, la goroutine corrente diventa responsabile per l'invocazione della funzione lenta, aggiornando la \verb"entry", e invia in broadcast la nuova \verb"entry" letta alle altre goroutine.
    \hfill \vspace{12pt}

    Si noti che \verb"e.res.value" e \verb"e.res.err" sono condivise fra molteplici goroutine.
    La goroutine che ha creato la \verb"entry" imposta il loro valore e le altre goroutine leggono il loro valore quando la condizione di ready sarà inviata.
    La chiusura del channel \verb"ready" \textit{avviene prima} della ricezione dell'evento da parte delle altre goroutine, quindi la scrittura di queste variabili nella prima goroutine \textit{avviene prima} che essi siano letti da qualunque successiva goroutine.
    Non ci sono data race.
    \hfill \vspace{12pt}

    L'ultima implementazione di \verb"Memo" usa i mutex a guardia di una variabile map che è condivisa da ogni goroutine che chiama \verb"Get".
    \hfill \vspace{12pt}

    Si veda ora un'altra scelta di implementazione della cache, quella con la variabile map confinata ad una \textit{goroutine monitor} a cui ogni chiamata di \verb"Get" deve inviare un messaggio.
    \hfill \vspace{12pt}

    La dichiarazione di \verb"Func", \verb"result" e \verb"entry" rimane come prima.
    Comunque, il tipo \verb"Memo" consiste ora di un channel \verb"requests" tramite il quale il chiamante di \verb"Get" comunica con la goroutine monitor.
    Il tipo elementare del channel è una \verb"request".
    Usando questa struttura, il chiamante di \verb"Get" invia alla goroutine monitor sia la chiave che l'argomento della funzione memoizzata, e un altro channel \verb"response" sopra il quale il risultato deve essere inviato indietro quando diventa disponibile.
    Questo channel porta solo un singolo valore.
    \begin{lstlisting}[frame = single,label={lst:lstlisting9-7.8}]
// Una richiesta %*\textit{è}*) un messaggio che richiede che Func sia applicato a key.
type request struct {
    key      string
    response chan<- result // il client vuole un risultato singolo
}

type Memo struct{ requests chan request }

// New ritorna una memoizzazione di f.
// I client devono successivamente chiamare Close.
func New(f Func) *Memo {
    memo := &Memo{requests: make(chan request)}
    go memo.server(f)
    return memo
}

func (memo *Memo) Get(key string) (interface{}, error) {
    response := make(chan result)
    memo.requests <- request{key, response}
    res := <-response
    return res.value, res.err
}

func (memo *Memo) Close() { close(memo.requests) }
    \end{lstlisting}
    Il metodo \verb"Get" crea un channel di risposta, lo mette nella richiesta, lo invia alla goroutine monitor e quindi riceve da esso.
    \hfill \vspace{12pt}

    La variabile di \verb"cache" è confinata alla goroutine monitor \verb"(*Memo).server".
    Il monitor legge richieste in un ciclo fino a quando il channel request viene chiuso dal metodo \verb"Close".
    Per ogni richiesta, esso consulta la cache, crea una nuova \verb"entry" se nessuno la trova e la salva.
    \begin{lstlisting}[frame = single,label={lst:lstlisting9-7.9}]
func (memo *Memo) server(f Func) {
    cache := make(map[string]*entry)
    for req := range memo.requests {
        e := cache[req.key]
        if e == nil {
            // Questa %*\textit{è}*) la prima richiesta per key.
            e = &entry{ready: make(chan struct{})}
            cache[req.key] = e
            go e.call(f, req.key) // chiama f(key)
        }
        go e.deliver(req.response)
    }
}

func (e *entry) call(f Func, key string) {
    // Valuta la funzione.
    e.res.value, e.res.err = f(key)
    // Invia in broadcast la condizione ready.
    close(e.ready)
}

func (e *entry) deliver(response chan<- result) {
    // Attende per la condizione ready.
    <-e.ready
    // Invia il risultato al client.
    response <- e.res
}
    \end{lstlisting}
    In un modo simile alla versione basata sul mutex, la prima richiesta per una data chiave diventa responsabile di chiamare la funzione \verb"f" su quella chiave, memorizzando il risultato nella \verb"entry" e facendo il broadcast sulla prontezza della \verb"entry" chiudendo il channel \verb"ready".
    Questo è fatto con \verb"(*entry).call".
    \hfill \vspace{12pt}

    Una successiva richiesta per la stessa chiave trova la \verb"entry" esistente nella map, attende che il risultato diventi ready e invia il risultato tramite la channel di response alla goroutine client che chiama \verb"Get".
    Questo è fatto da \verb"(*entry).deliver".
    I metodi \verb"call" e \verb"deliver" sono chiamati nella loro goroutine per assicurare che la goroutine monitor non fermi il processo di nuove richieste.
    \hfill \vspace{12pt}

    Questo esempio mostra che è possibile costruire molte strutture concorrenti usando entrambi gli approcci: variabili condivise e lock o communicating sequential processes.
    \hfill \vspace{12pt}

    Non è sempre ovvio quale approccio è preferibile in una data situazione, ma non conviene conoscere come può essere implementato nei due stili perché non è sempre facile capire come tradurre le proprie scelte progettuali in entrambi gli approcci.
    Nonostante questo, qualche volta passare da un approccio all'altro può anche rendere il proprio codice più semplice.
\end{document}